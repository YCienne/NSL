# NSL
This project is a real-time sign language detection system that translates sign language gestures into text, enhancing communication between deaf and hearing individuals. Built with Python and TensorFlow, it uses advanced deep learning models, including Xception, InceptionV3, and DenseNet121, with attention mechanisms and Gaussian noise for accurate gesture recognition.

Key features:

Data Processing: Processes image data using ImageDataGenerator for normalization and augmentation, which improves model reliability and generalization.
Model Architecture: Combines pre-trained networks (Xception, InceptionV3, DenseNet121) with custom attention and noise layers to enhance feature extraction, classifying 21 unique sign language gestures.
Evaluation: Validated on test sets, generating classification reports to ensure accuracy and assess performance across gestures.
This project demonstrates the role of computer vision and deep learning in making communication more accessible, providing a practical tool for bridging language gaps.
